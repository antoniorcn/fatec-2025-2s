{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4e26e5-53b4-4752-8ecc-a1c856e23268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45095931-5d0e-4925-b508-019e0d214467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62df2d49-c70e-44b1-af25-b1068809f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e17d91c-fd65-428a-b32b-f68c408812ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3febb68d-3209-4443-b9f8-a49ce180a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f179c34-f03a-4b8e-97c9-5c8dd5bf73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5ddd4e9-c3eb-4b96-8cbe-fb4c13b3a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0220c08a-4689-4a24-984b-e34b48ff9953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "language = \"portuguese\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bae312b-1874-4906-952d-03a5e0466b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = KeyedVectors.load_word2vec_format(\"../../../dados/skip_s100/skip_s100.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa3b617c-c2ea-4b84-9203-364c83e1d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>Mais uma vez, o Sr. Costner arrumou um filme p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>Este é um exemplo do motivo pelo qual a maiori...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>Primeiro de tudo eu odeio esses raps imbecis, ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>Nem mesmo os Beatles puderam escrever músicas ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brass pictures movies is not a fitting word fo...</td>\n",
       "      <td>Filmes de fotos de latão não é uma palavra apr...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49454</th>\n",
       "      <td>49456</td>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>Como a média de votos era muito baixa, e o fat...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49455</th>\n",
       "      <td>49457</td>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>O enredo teve algumas reviravoltas infelizes e...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49456</th>\n",
       "      <td>49458</td>\n",
       "      <td>I am amazed at how this movieand most others h...</td>\n",
       "      <td>Estou espantado com a forma como este filme e ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49457</th>\n",
       "      <td>49459</td>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>A Christmas Together realmente veio antes do m...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49458</th>\n",
       "      <td>49460</td>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>O drama romântico da classe trabalhadora do di...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49459 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                            text_en  \\\n",
       "0          1  Once again Mr. Costner has dragged out a movie...   \n",
       "1          2  This is an example of why the majority of acti...   \n",
       "2          3  First of all I hate those moronic rappers, who...   \n",
       "3          4  Not even the Beatles could write songs everyon...   \n",
       "4          5  Brass pictures movies is not a fitting word fo...   \n",
       "...      ...                                                ...   \n",
       "49454  49456  Seeing as the vote average was pretty low, and...   \n",
       "49455  49457  The plot had some wretched, unbelievable twist...   \n",
       "49456  49458  I am amazed at how this movieand most others h...   \n",
       "49457  49459  A Christmas Together actually came before my t...   \n",
       "49458  49460  Working-class romantic drama from director Mar...   \n",
       "\n",
       "                                                 text_pt sentiment  \n",
       "0      Mais uma vez, o Sr. Costner arrumou um filme p...       neg  \n",
       "1      Este é um exemplo do motivo pelo qual a maiori...       neg  \n",
       "2      Primeiro de tudo eu odeio esses raps imbecis, ...       neg  \n",
       "3      Nem mesmo os Beatles puderam escrever músicas ...       neg  \n",
       "4      Filmes de fotos de latão não é uma palavra apr...       neg  \n",
       "...                                                  ...       ...  \n",
       "49454  Como a média de votos era muito baixa, e o fat...       pos  \n",
       "49455  O enredo teve algumas reviravoltas infelizes e...       pos  \n",
       "49456  Estou espantado com a forma como este filme e ...       pos  \n",
       "49457  A Christmas Together realmente veio antes do m...       pos  \n",
       "49458  O drama romântico da classe trabalhadora do di...       pos  \n",
       "\n",
       "[49459 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../../dados/nlp/imdb-reviews-pt-br.csv\", encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660ac518-cdce-4be6-9767-ba5ad0b86181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_en</th>\n",
       "      <th>text_pt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, text_en, text_pt, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna( inplace=True )\n",
    "df[df[\"text_pt\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce15ea-9f1b-48ac-9c87-073ddadcfbac",
   "metadata": {},
   "source": [
    "50000 frases\n",
    "50 palavras por frase\n",
    "100 dimensoes por palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3d4f107c-841b-489c-b271-3111a9873177",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PALAVRAS = 50\n",
    "DIMENSOES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2b455d94-14d6-4565-9965-1c71ab8b9dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.4185791015625"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoria = (50000 * MAX_PALAVRAS * DIMENSOES) / 1024 / 1024 \n",
    "memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ea33f210-282d-4cb2-8ae9-dd223fb18229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anton\\AppData\\Local\\Temp\\ipykernel_28732\\2064889501.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"output\"] = df[\"sentiment\"].replace( {\"neg\": 0, \"pos\": 1} )\n"
     ]
    }
   ],
   "source": [
    "df[\"output\"] = df[\"sentiment\"].replace( {\"neg\": 0, \"pos\": 1} ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6ee84f9f-f082-4271-889e-38469253b5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords :  ['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "lista_stopwords = [s for s in stopwords.words(language)]\n",
    "translator = str.maketrans('', '', string.punctuation + '\\u200b' )\n",
    "print(\"Stopwords : \", lista_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "508d1fb1-7a9a-4ce1-86f8-9ff678a04bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_texto( texto ):\n",
    "    global counter, lista_stopwords, translator\n",
    "    texto_limpo = texto.lower().translate(translator)\n",
    "    tokens = word_tokenize( texto_limpo )\n",
    "    tokens_limpos = []\n",
    "    if counter % 500 == 0:\n",
    "        print(\"Limpando frases \", counter )\n",
    "    for token in tokens: \n",
    "        if token not in lista_stopwords:\n",
    "            tokens_limpos.append( token )\n",
    "    counter += 1\n",
    "    return tokens_limpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0581949c-7037-4332-8680-459813722e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:  Este é um exemplo do motivo pelo qual a maioria dos filmes de ação são os mesmos. Genérico e chato, não há nada que valha a pena assistir aqui. Um completo desperdício dos talentos de Ice-T e Cubo de Gelo que foram mal aproveitados, cada um comprovando que são capazes de atuar e agir bem. Não se incomode com este, vá ver New Jack City, Ricochet ou assistir New York Undercover para Ice-T, ou Boyz no Hood, Higher Learning ou Friday for Ice Cube e ver o negócio real. Ice-Ts horrivelmente clichê diálogo sozinho faz este filme ralar os dentes, e eu ainda estou me perguntando o que diabos Bill Paxton estava fazendo neste filme? E por que diabos ele sempre interpreta exatamente o mesmo personagem? Dos extraterrestres em diante, todos os filmes que eu vi com Bill Paxton o fizeram interpretar exatamente o mesmo personagem irritante, e pelo menos em Aliens seu personagem morreu, o que o tornou um pouco gratificante ... No geral, esse é lixo de ação de segunda classe. Existem incontáveis ​​filmes melhores para ver, e se você realmente quiser ver esse filme, assista a Judgment Night, que é praticamente uma cópia carbono, mas tem melhor atuação e um roteiro melhor. A única coisa que fez isso valer a pena assistir foi uma mão decente na câmera - a cinematografia era quase refrescante, o que chega perto de compensar o horrível filme em si - mas não é bem assim. 4/10\n",
      "Limpando frases  0\n",
      "\n",
      "\n",
      "Tokens Limpos:  ['exemplo', 'motivo', 'maioria', 'filmes', 'ação', 'mesmos', 'genérico', 'chato', 'nada', 'valha', 'pena', 'assistir', 'aqui', 'completo', 'desperdício', 'talentos', 'icet', 'cubo', 'gelo', 'mal', 'aproveitados', 'cada', 'comprovando', 'capazes', 'atuar', 'agir', 'bem', 'incomode', 'vá', 'ver', 'new', 'jack', 'city', 'ricochet', 'assistir', 'new', 'york', 'undercover', 'icet', 'boyz', 'hood', 'higher', 'learning', 'friday', 'ice', 'cube', 'ver', 'negócio', 'real', 'icets', 'horrivelmente', 'clichê', 'diálogo', 'sozinho', 'faz', 'filme', 'ralar', 'dentes', 'ainda', 'perguntando', 'diabos', 'bill', 'paxton', 'fazendo', 'neste', 'filme', 'diabos', 'sempre', 'interpreta', 'exatamente', 'personagem', 'extraterrestres', 'diante', 'todos', 'filmes', 'vi', 'bill', 'paxton', 'fizeram', 'interpretar', 'exatamente', 'personagem', 'irritante', 'menos', 'aliens', 'personagem', 'morreu', 'tornou', 'pouco', 'gratificante', 'geral', 'lixo', 'ação', 'segunda', 'classe', 'existem', 'incontáveis', 'filmes', 'melhores', 'ver', 'realmente', 'quiser', 'ver', 'filme', 'assista', 'judgment', 'night', 'praticamente', 'cópia', 'carbono', 'melhor', 'atuação', 'roteiro', 'melhor', 'única', 'coisa', 'fez', 'valer', 'pena', 'assistir', 'mão', 'decente', 'câmera', 'cinematografia', 'quase', 'refrescante', 'chega', 'perto', 'compensar', 'horrível', 'filme', 'si', 'bem', 'assim', '410']\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto Original: \", df[\"text_pt\"][1])\n",
    "print(\"\\n\\nTokens Limpos: \", limpar_texto(df[\"text_pt\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d2955218-3479-49a1-a142-eb34c9b47764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpando frases  500\n",
      "Limpando frases  1000\n",
      "Limpando frases  1500\n",
      "Limpando frases  2000\n",
      "Limpando frases  2500\n",
      "Limpando frases  3000\n",
      "Limpando frases  3500\n",
      "Limpando frases  4000\n",
      "Limpando frases  4500\n",
      "Limpando frases  5000\n",
      "Limpando frases  5500\n",
      "Limpando frases  6000\n",
      "Limpando frases  6500\n",
      "Limpando frases  7000\n",
      "Limpando frases  7500\n",
      "Limpando frases  8000\n",
      "Limpando frases  8500\n",
      "Limpando frases  9000\n",
      "Limpando frases  9500\n",
      "Limpando frases  10000\n",
      "Limpando frases  10500\n",
      "Limpando frases  11000\n",
      "Limpando frases  11500\n",
      "Limpando frases  12000\n",
      "Limpando frases  12500\n",
      "Limpando frases  13000\n",
      "Limpando frases  13500\n",
      "Limpando frases  14000\n",
      "Limpando frases  14500\n",
      "Limpando frases  15000\n",
      "Limpando frases  15500\n",
      "Limpando frases  16000\n",
      "Limpando frases  16500\n",
      "Limpando frases  17000\n",
      "Limpando frases  17500\n",
      "Limpando frases  18000\n",
      "Limpando frases  18500\n",
      "Limpando frases  19000\n",
      "Limpando frases  19500\n",
      "Limpando frases  20000\n",
      "Limpando frases  20500\n",
      "Limpando frases  21000\n",
      "Limpando frases  21500\n",
      "Limpando frases  22000\n",
      "Limpando frases  22500\n",
      "Limpando frases  23000\n",
      "Limpando frases  23500\n",
      "Limpando frases  24000\n",
      "Limpando frases  24500\n",
      "Limpando frases  25000\n",
      "Limpando frases  25500\n",
      "Limpando frases  26000\n",
      "Limpando frases  26500\n",
      "Limpando frases  27000\n",
      "Limpando frases  27500\n",
      "Limpando frases  28000\n",
      "Limpando frases  28500\n",
      "Limpando frases  29000\n",
      "Limpando frases  29500\n",
      "Limpando frases  30000\n",
      "Limpando frases  30500\n",
      "Limpando frases  31000\n",
      "Limpando frases  31500\n",
      "Limpando frases  32000\n",
      "Limpando frases  32500\n",
      "Limpando frases  33000\n",
      "Limpando frases  33500\n",
      "Limpando frases  34000\n",
      "Limpando frases  34500\n",
      "Limpando frases  35000\n",
      "Limpando frases  35500\n",
      "Limpando frases  36000\n",
      "Limpando frases  36500\n",
      "Limpando frases  37000\n",
      "Limpando frases  37500\n",
      "Limpando frases  38000\n",
      "Limpando frases  38500\n",
      "Limpando frases  39000\n",
      "Limpando frases  39500\n",
      "Limpando frases  40000\n",
      "Limpando frases  40500\n",
      "Limpando frases  41000\n",
      "Limpando frases  41500\n",
      "Limpando frases  42000\n",
      "Limpando frases  42500\n",
      "Limpando frases  43000\n",
      "Limpando frases  43500\n",
      "Limpando frases  44000\n",
      "Limpando frases  44500\n",
      "Limpando frases  45000\n",
      "Limpando frases  45500\n",
      "Limpando frases  46000\n",
      "Limpando frases  46500\n",
      "Limpando frases  47000\n",
      "Limpando frases  47500\n",
      "Limpando frases  48000\n",
      "Limpando frases  48500\n",
      "Limpando frases  49000\n"
     ]
    }
   ],
   "source": [
    "df[\"X\"] = df[\"text_pt\"].apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "61654b58-b5d2-41e5-a08e-43ddbb85ed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vazio = np.zeros( (DIMENSOES,) ).tolist()\n",
    "token_vazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1aee2b62-57e7-4267-966e-7e3e03a533a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vetorizar_tokens( tokens ):\n",
    "    global MAX_PALAVRAS, token_vazio\n",
    "    lista_tokens_embed = []\n",
    "    count_token = 0 \n",
    "    for token in tokens:\n",
    "        try: \n",
    "            lista_tokens_embed.append( word2vec_model[token] )\n",
    "            count_token += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if count_token >= MAX_PALAVRAS:\n",
    "            break\n",
    "\n",
    "    for i in range(count_token, MAX_PALAVRAS, 1):\n",
    "        lista_tokens_embed.append( token_vazio )\n",
    "    return lista_tokens_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3b0db39f-1c94-4142-8410-47c51e1cd2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = vetorizar_tokens( df[\"X\"][1][0:35] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3ffb958c-cbdd-4905-9c0e-cb59fdd2c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"X_VET\"] = df[\"X\"].apply( vetorizar_tokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee39114c-9d3b-48bf-91a6-e55efb0a0214",
   "metadata": {},
   "source": [
    "exemplo ==> [0.3123, 0.6123, 0.2342, 0.23423, 0.4564, ... ]\n",
    "refrescante ==> [0.1234, 0.2234, 0.6234, 0.1234, ... ]\n",
    "X[0] ==> [\n",
    "    [0.3123, 0.6123, 0.2342, 0.23423, 0.4564, ... ],\n",
    "    [0.1234, 0.2234, 0.6234, 0.1234, ... ],\n",
    "    ...  # PALAVRAS_MAX\n",
    "    [0.0, 0.0, 0.0, 0.0, ... ]\n",
    "]\n",
    "\n",
    "Flatten ==> \n",
    "[0.3123, 0.6123, 0.2342, 0.23423, 0.4564, ... , 0.1234, 0.2234, 0.6234, 0.1234, ..., 0.0, 0.0, 0.0, 0.0, ...   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b1a910d4-5d4f-4906-bb89-f0eb54d22a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"X_VET\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "162167cd-72a8-48ed-9f64-5b93a6012322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten( lista_vetores ):\n",
    "    arr = np.array(lista_vetores)\n",
    "    return arr.flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f7b096ae-4b6d-4586-b4ed-ded127c40f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"X_VET\"].apply( flatten )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4f1decda-434b-40f1-8bff-e20ae8bee918",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "fc171bde-054c-47b6-b63d-a1f2ca7aea32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49459,)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "6739af49-6d5b-4a03-bca1-1cc8c7786435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array( X ).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "dd86eba1-25cb-4524-bebf-9ef8e95a58b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.to_numpy().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6195fba8-92e0-4097-849b-bbd9f48ae561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 100, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ea0867d2-6d4d-475c-8e74-b35b514c86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = np.array(X_train.to_list())\n",
    "Y_train_np = np.array(Y_train.to_list())\n",
    "X_test_np = np.array(X_test.to_list())\n",
    "Y_test_np = np.array(Y_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d6c4216a-7a95-4081-b072-03f9e2dfb3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,121,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m5,121,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │           \u001b[38;5;34m2,064\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,254,305</span> (20.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,254,305\u001b[0m (20.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,254,305</span> (20.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,254,305\u001b[0m (20.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelo = Sequential()\n",
    "modelo.add( Input( (DIMENSOES * PALAVRAS_MAX, ) ) )\n",
    "modelo.add( Dense( 1024, activation=\"relu\" ) )\n",
    "modelo.add( Dense( 128, activation=\"relu\" ) )\n",
    "modelo.add( Dense( 16, activation=\"relu\" ) )\n",
    "modelo.add( Dense( 1, activation=\"sigmoid\" ) )\n",
    "# modelo.add( Dense( 3, activation=\"softmax\" ) )\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d97683a1-accf-4f7f-b1cc-f75f50da7b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile( optimizer = \"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"] )\n",
    "# modelo.compile( optimizer = \"adam\", loss=\"categorial_crossentropy\", metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "db968ac1-a5c6-4eb6-b3e3-4515738b8451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 248ms/step - accuracy: 0.6275 - loss: 0.6401\n",
      "Epoch 2/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 158ms/step - accuracy: 0.7403 - loss: 0.5174\n",
      "Epoch 3/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 129ms/step - accuracy: 0.8690 - loss: 0.3060\n",
      "Epoch 4/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 125ms/step - accuracy: 0.9812 - loss: 0.0613\n",
      "Epoch 5/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 104ms/step - accuracy: 0.9924 - loss: 0.0236\n",
      "Epoch 6/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 104ms/step - accuracy: 0.9914 - loss: 0.0246\n",
      "Epoch 7/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 106ms/step - accuracy: 0.9888 - loss: 0.0301\n",
      "Epoch 8/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 112ms/step - accuracy: 0.9913 - loss: 0.0229\n",
      "Epoch 9/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 127ms/step - accuracy: 0.9945 - loss: 0.0169\n",
      "Epoch 10/10\n",
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 208ms/step - accuracy: 0.9950 - loss: 0.0145\n"
     ]
    }
   ],
   "source": [
    "historico = modelo.fit( X_train_np, Y_train_np, epochs = 10, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f512dd2d-7f0f-4065-bacc-1a0062c30d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m310/310\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 39ms/step - accuracy: 0.6630 - loss: 1.7910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8196674585342407, 0.6583097577095032]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.evaluate( X_test_np, Y_test_np )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "366b5e53-3837-419d-9c17-8e3175199d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['odeio', 'senhor', 'aneis']"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto = \"odeio o senhor dos aneis\"\n",
    "tokens_limpos = limpar_texto( texto )\n",
    "tokens_limpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "fe41fbfc-102a-4d63-bf64-8a1f5886ab9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_flatten = flatten(vetorizar_tokens( tokens_limpos ))\n",
    "tokens_array = np.array([np.array(tokens_flatten)]).astype(np.float32)\n",
    "tokens_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "934d010a-f83e-4f03-a215-52ef06bafd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "7ddab77e-8c00-4f98-b93e-a5bfd1180b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.239889  , -0.14567301,  0.07306   , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_array.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1ac9b521-b04d-41e7-85bd-57d6691a833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positivo'"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"positivo\" if modelo.predict( tokens_array )[0][0] >= 0.5 else \"negativo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c431c-20e6-4835-945b-305b97ae43f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "positivo ==> [0, 0, 1]\n",
    "negativo ==> [0, 1, 0]\n",
    "neutro ==> [1, 0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
